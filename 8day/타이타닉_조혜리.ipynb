{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "411c29af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3. 독립변수 : alone, pclass\\n   종속변수 : survived \\n   => 동행자 유무와 객실등급에 따른 생존 사망 (이진분류)\\n원핫인코딩 ex) 성별컬럼 -> 여자컬럼 남자컬럼'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''3. 독립변수 : alone, pclass\n",
    "   종속변수 : survived \n",
    "   => 동행자 유무와 객실등급에 따른 생존 사망 (이진분류)\n",
    "원핫인코딩 ex) 성별컬럼 -> 여자컬럼 남자컬럼'''\n",
    "## 정규화는 해주는게 좋다 범주형 데이터라도 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ae10b38c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff4dece",
   "metadata": {},
   "source": [
    "### 타이타닉 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "573ff5bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 3 columns):\n",
      " #   Column    Non-Null Count  Dtype\n",
      "---  ------    --------------  -----\n",
      " 0   survived  891 non-null    int64\n",
      " 1   pclass    891 non-null    int64\n",
      " 2   alone     891 non-null    bool \n",
      "dtypes: bool(1), int64(2)\n",
      "memory usage: 14.9 KB\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('./data/06_(문제)타이타닉_분류_데이터셋.csv')\n",
    "data=data[['survived','pclass','alone']]\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "491302f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "31e7ff22",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>alone</th>\n",
       "      <th>1_class</th>\n",
       "      <th>2_class</th>\n",
       "      <th>3_class</th>\n",
       "      <th>alone_F</th>\n",
       "      <th>alone_T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>False</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived  pclass  alone  1_class  2_class  3_class  alone_F  alone_T\n",
       "0           0       3  False      0.0      0.0      1.0      1.0      0.0\n",
       "1           1       1  False      1.0      0.0      0.0      1.0      0.0\n",
       "2           1       3   True      0.0      0.0      1.0      0.0      1.0\n",
       "3           1       1  False      1.0      0.0      0.0      1.0      0.0\n",
       "4           0       3   True      0.0      0.0      1.0      0.0      1.0\n",
       "..        ...     ...    ...      ...      ...      ...      ...      ...\n",
       "886         0       2   True      0.0      1.0      0.0      0.0      1.0\n",
       "887         1       1   True      1.0      0.0      0.0      0.0      1.0\n",
       "888         0       3  False      0.0      0.0      1.0      1.0      0.0\n",
       "889         1       1   True      1.0      0.0      0.0      0.0      1.0\n",
       "890         0       3   True      0.0      0.0      1.0      0.0      1.0\n",
       "\n",
       "[891 rows x 8 columns]"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_class = pd.DataFrame(OneHotEncoder().fit_transform(data['pclass'].values[:,np.newaxis]).toarray(),\n",
    "                    columns = ['1_class','2_class','3_class'],\n",
    "                    index = data.index)\n",
    "data_alone = pd.DataFrame(OneHotEncoder().fit_transform(data['alone'].values[:,np.newaxis]).toarray(),\n",
    "                    columns = ['alone_F','alone_T'],\n",
    "                    index = data.index)\n",
    "data = pd.concat([data,data_class],axis=1)\n",
    "data = pd.concat([data,data_alone],axis=1)\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "18422c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>1_class</th>\n",
       "      <th>2_class</th>\n",
       "      <th>3_class</th>\n",
       "      <th>alone_F</th>\n",
       "      <th>alone_T</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>886</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>887</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>888</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>890</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>891 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     survived  1_class  2_class  3_class  alone_F  alone_T\n",
       "0           0      0.0      0.0      1.0      1.0      0.0\n",
       "1           1      1.0      0.0      0.0      1.0      0.0\n",
       "2           1      0.0      0.0      1.0      0.0      1.0\n",
       "3           1      1.0      0.0      0.0      1.0      0.0\n",
       "4           0      0.0      0.0      1.0      0.0      1.0\n",
       "..        ...      ...      ...      ...      ...      ...\n",
       "886         0      0.0      1.0      0.0      0.0      1.0\n",
       "887         1      1.0      0.0      0.0      0.0      1.0\n",
       "888         0      0.0      0.0      1.0      1.0      0.0\n",
       "889         1      1.0      0.0      0.0      0.0      1.0\n",
       "890         0      0.0      0.0      1.0      0.0      1.0\n",
       "\n",
       "[891 rows x 6 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[['survived','1_class','2_class','3_class','alone_F','alone_T']]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "02ef83b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "target = data['survived'].to_numpy()\n",
    "data_input = data[['1_class','2_class','3_class','alone_F','alone_T']].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6347464d",
   "metadata": {},
   "source": [
    "### 훈련데이터와 테스트 데이터 분리하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cd6a6f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "5bca687f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, test_input, train_target,test_target = train_test_split(data_input,target,random_state=37)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1cd45c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(668, 5)\n",
      "(668,)\n",
      "(223, 5)\n",
      "(223,)\n"
     ]
    }
   ],
   "source": [
    "print(train_input.shape)\n",
    "print(train_target.shape)\n",
    "print(test_input.shape)\n",
    "print(test_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae37117",
   "metadata": {},
   "source": [
    "## KNN 분류모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "6c96739a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7095808383233533\n",
      "0.6547085201793722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "kn=KNeighborsClassifier()\n",
    "kn.fit(train_input, train_target)\n",
    "print(kn.score(train_input, train_target))\n",
    "print(kn.score(test_input, test_target))\n",
    "\n",
    "# [해석]\n",
    "# 훈련데이터와 테스트데이터 결정계수 0.05차이\n",
    "# 약간 과대적합이지만\n",
    "# 전체적으로 결과가 낮은 과소적합을 보이고 있다 \n",
    "# 정확도가 낮아 사용할 수 없다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9d582f10",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "knn 훈련모델\n",
      "테스트 :  0.6547085201793722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "kn=KNeighborsClassifier(n_neighbors=6)\n",
    "kn.fit(train_input, train_target)\n",
    "print('knn 훈련모델')\n",
    "print('테스트 : ',kn.score(test_input, test_target))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "351eb353",
   "metadata": {},
   "source": [
    "## 교차검증 KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "41636b44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(712, 5) (712,)\n",
      "(179, 5) (179,)\n"
     ]
    }
   ],
   "source": [
    "train_input_cross, test_input_cross, train_target_cross,test_target_cross = train_test_split(data_input,target,test_size=0.2,random_state=37)\n",
    "print(train_input_cross.shape,train_target_cross.shape)\n",
    "print(test_input_cross.shape,test_target_cross.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4c22545e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 5) (569,)\n",
      "(143, 5) (143,)\n"
     ]
    }
   ],
   "source": [
    "sub_input, val_input, sub_target,val_target = train_test_split(train_input_cross,train_target_cross,test_size=0.2,random_state=37)\n",
    "print(sub_input.shape,sub_target.shape)\n",
    "print(val_input.shape,val_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1870cf2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6889279437609842\n",
      "0.6573426573426573\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    " # 괄호안에 넣는 값 사람이 지정 : 하이퍼파라미터, 정확도 높이기 위해 찾아내야한다 \n",
    "kn.fit(sub_input, sub_target)\n",
    "print(kn.score(sub_input, sub_target))\n",
    "print(kn.score(val_input,val_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c9f6fe5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.00299311, 0.00098801, 0.00112557, 0.00099683, 0.00099707]), 'score_time': array([0.01196718, 0.00598478, 0.00685048, 0.00398898, 0.00598598]), 'test_score': array([0.73426573, 0.71328671, 0.68309859, 0.6971831 , 0.71126761])}\n",
      "\n",
      "\n",
      "최종 훈련모델 ;  0.7078203486654191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "### 교차검증 모듈 : cross_validate\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# dtc : 결정트리 훈련모델(다른모델을 사용한 경우 해당모델 적어주면된다)\n",
    "# 두번째값 : 훈련데이터(fold에서 훈련데이터 쪼갤때 사용)\n",
    "# 세번째값 : 검증데이터(fold에서 검증데이터 쪼갤때 사용)\n",
    "scores = cross_validate(kn,train_input_cross,train_target_cross)\n",
    "print(scores)\n",
    "# 기본값은 5폴드\n",
    "# test_score : 각 겹의 평균값\n",
    "\n",
    "## 딕셔너리의 test_score의 평균값이 최종 훈련모델의 평가점수(정확도)\n",
    "\n",
    "# 최종 훈련모델의 성능\n",
    "print('\\n')\n",
    "print('최종 훈련모델 ; ',scores['test_score'].mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "59fc862e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 :  0.7078203486654191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "### 훈련데이터를 섞거나, 폴드의 갯수를 지정할 수 있는 클래스\n",
    "# - StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 교차검증 함수 그대로 사용\n",
    "# cv : 분할기 속성\n",
    "#    : 분할기로 StratifiedKFold 클래스 사용\n",
    "#    : 속성값이 없을 경우 기본 fold 는 5, 기본 섞지는 않음 \n",
    "scores = cross_validate(kn,train_input_cross, train_target_cross, cv=StratifiedKFold())\n",
    "\n",
    "print('최종 : ',np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227b4ba0",
   "metadata": {},
   "source": [
    "## 로지스틱 이진분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "08b664ba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로지스틱 회귀분류\n",
      "훈련 :  0.7095808383233533\n",
      "테스트 :  0.6547085201793722\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(train_input,train_target)\n",
    "\n",
    "print('로지스틱 회귀분류')\n",
    "print('훈련 : ' ,lr.score(train_input, train_target))\n",
    "print('테스트 : ',lr.score(test_input, test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "776aa22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7223198594024605\n",
      "0.6643356643356644\n"
     ]
    }
   ],
   "source": [
    "# 괄호안에 넣는 값 사람이 지정 : 하이퍼파라미터, 정확도 높이기 위해 찾아내야한다 \n",
    "lr.fit(sub_input, sub_target)\n",
    "print(lr.score(sub_input, sub_target))\n",
    "print(lr.score(val_input,val_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ee566e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.00952554, 0.00598502, 0.00797749, 0.00697541, 0.01096511]), 'score_time': array([0.00059938, 0.00102186, 0.00100136, 0.00100231, 0.00099635]), 'test_score': array([0.73426573, 0.71328671, 0.68309859, 0.6971831 , 0.70422535])}\n",
      "\n",
      "\n",
      "최종 훈련모델 ;  0.7064118979611937\n"
     ]
    }
   ],
   "source": [
    "### 교차검증 모듈 : cross_validate\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# dtc : 결정트리 훈련모델(다른모델을 사용한 경우 해당모델 적어주면된다)\n",
    "# 두번째값 : 훈련데이터(fold에서 훈련데이터 쪼갤때 사용)\n",
    "# 세번째값 : 검증데이터(fold에서 검증데이터 쪼갤때 사용)\n",
    "scores = cross_validate(lr,train_input_cross,train_target_cross)\n",
    "print(scores)\n",
    "# 기본값은 5폴드\n",
    "# test_score : 각 겹의 평균값\n",
    "\n",
    "## 딕셔너리의 test_score의 평균값이 최종 훈련모델의 평가점수(정확도)\n",
    "\n",
    "# 최종 훈련모델의 성능\n",
    "print('\\n')\n",
    "print('최종 훈련모델 ; ',scores['test_score'].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "719c86b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 :  0.7092862461440271\n"
     ]
    }
   ],
   "source": [
    "### 훈련데이터를 섞거나, 폴드의 갯수를 지정할 수 있는 클래스\n",
    "# - StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 교차검증 함수 그대로 사용\n",
    "# cv : 분할기 속성\n",
    "#    : 분할기로 StratifiedKFold 클래스 사용\n",
    "#    : 속성값이 없을 경우 기본 fold 는 5, 기본 섞지는 않음 \n",
    "splitter = StratifiedKFold(n_splits=3,shuffle = True,random_state=37)\n",
    "scores = cross_validate(lr,train_input_cross, train_target_cross, cv=splitter)\n",
    "\n",
    "print('최종 : ',np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61263f78",
   "metadata": {},
   "source": [
    "## 결정트리 모델 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "303b0822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결정트리모델\n",
      "훈련 :  0.7095808383233533\n",
      "테스트 :  0.6547085201793722\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 클래스 생성하기\n",
    "# - 실제사용시에는 randon_state는 사용하지 않는것이 좋다\n",
    "# - random_state값이 변경되면 정확도의 점수도 변경된다\n",
    "# 훈련모델 생성 \n",
    "dtc = DecisionTreeClassifier(random_state=42)\n",
    "dtc.fit(train_input,train_target)\n",
    "\n",
    "# 훈련 및 테스트 데이터 정확도 확인하기\n",
    "print('결정트리모델')\n",
    "print('훈련 : ',dtc.score(train_input,train_target))\n",
    "print('테스트 : ' ,dtc.score(test_input,test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "ee5eaeec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(estimator=DecisionTreeClassifier(random_state=37), n_jobs=-1,\n",
       "             param_grid={&#x27;max_depth&#x27;: range(5, 20)})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(estimator=DecisionTreeClassifier(random_state=37), n_jobs=-1,\n",
       "             param_grid={&#x27;max_depth&#x27;: range(5, 20)})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=37)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(random_state=37)</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(estimator=DecisionTreeClassifier(random_state=37), n_jobs=-1,\n",
       "             param_grid={'max_depth': range(5, 20)})"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV \n",
    "\n",
    "\n",
    "# 찾을 하이퍼파라미터는 딕셔너리로\n",
    "# 실제 파라미터 변수이름 그대로 찾을 범위 지정\n",
    "params = {'max_depth' : range(5,20,1)}\n",
    "\n",
    "\n",
    "#객체(모델)생성\n",
    "# - 첫번째 값: 훈련모델 넣기\n",
    "# - 두번째 값 : 찾을 하이퍼파라미터 값들(딕셔너리로 정의)\n",
    "# - 세번째 값 : CPU코어 갯수(-1은 모든 코어 사용, 병렬처리)\n",
    "\n",
    "gs = GridSearchCV(DecisionTreeClassifier(random_state=37),params, n_jobs=-1)\n",
    "\n",
    "# 훈련시키기\n",
    "gs.fit(train_input, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "78bdc2be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7095808383233533\n",
      "0.6547085201793722\n"
     ]
    }
   ],
   "source": [
    "dtc = gs.best_estimator_\n",
    "print(dtc.score(train_input,train_target))\n",
    "print(dtc.score(test_input, test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ac347f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 5}\n"
     ]
    }
   ],
   "source": [
    "print(gs.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d254b5",
   "metadata": {},
   "source": [
    "## 확률적 경사 하강법(SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2a3de2f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "확률적 경사하강법(SGD)\n",
      "훈련 :  0.6631736526946108\n",
      "테스트 :  0.6816143497757847\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### 사용클래스(모델) : SGDClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# 클래스(모델)생성\n",
    "sc = SGDClassifier(loss='log', max_iter=10, random_state=42)\n",
    "\n",
    "# 훈련시키기\n",
    "sc.fit(train_input,train_target)\n",
    "\n",
    "# \n",
    "print('확률적 경사하강법(SGD)')\n",
    "print('훈련 : ',sc.score(train_input, train_target))\n",
    "print('테스트 : ',sc.score(test_input,test_target))\n",
    "\n",
    "# [해석]\n",
    "# 훈련 :  0.9812206572769953\n",
    "# 테스트 :  0.958041958041958\n",
    "# 0.03 차이 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "73dc6bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sc = SGDClassifier(loss='log',random_state =42)\n",
    "\n",
    "train_score = []\n",
    "test_score = []\n",
    "\n",
    "# 범주 고유값 \n",
    "classes = np.unique(train_target)\n",
    "# 반복을 300회 이상으로 테스트하여 정확도를 리스트에 저장 \n",
    "for _ in range(0,300):\n",
    "    sc.partial_fit(train_input, train_target,classes=classes)\n",
    "    \n",
    "    train_score.append(sc.score(train_input,train_target))\n",
    "    test_score.append(sc.score(test_input,test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "986bfb4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 최적의 에포크 위치 확인하기 : 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(train_score, label = 'train_score')\n",
    "plt.plot(test_score, label = 'test_score')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "58a348a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 :  0.7095808383233533\n",
      "테스트 :  0.6547085201793722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### 사용클래스(모델) : SGDClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# 클래스(모델)생성\n",
    "# tol=None 모델이 스스로 찾는 최적값을 무시하고  우리가 지정해준 max_iter 값까지 수행하도록 \n",
    "sc = SGDClassifier(loss='log', max_iter=150, tol=None, random_state=42)\n",
    "\n",
    "# 훈련시키기\n",
    "sc.fit(train_input,train_target)\n",
    "\n",
    "print\n",
    "print('훈련 : ',sc.score(train_input, train_target))\n",
    "print('테스트 : ',sc.score(test_input,test_target))\n",
    "\n",
    "# [해석]\n",
    "# 훈련 :  0.7095808383233533\n",
    "# 테스트 :  0.6547085201793722\n",
    "# 0.01 차이 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5599f24",
   "metadata": {},
   "source": [
    "## 트리의 앙상블 모델 \n",
    "### - 랜덤 포레스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "ab3ae9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트리의 앙상블 모델 (랜덤포레스트)\n",
      "0.7095810143862229\n",
      "0.7095836606441477\n"
     ]
    }
   ],
   "source": [
    "# 랜덤포레스트 객체생성 : 코어 모두 사용\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_jobs= -1 , random_state=37)\n",
    "\n",
    "# 교차검증 진행\n",
    "# - return_train_score : 검증결과 반환받기\n",
    "from sklearn.model_selection import cross_validate\n",
    "scores = cross_validate(rfc,train_input, train_target, return_train_score = True, n_jobs=-1)\n",
    "\n",
    "# 최종 훈련평가 결과 및 검증결과\n",
    "print('트리의 앙상블 모델 (랜덤포레스트)')\n",
    "print(np.mean(scores['train_score']))\n",
    "print(np.mean(scores['test_score']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29dd62eb",
   "metadata": {},
   "source": [
    "## 트리의 앙상블 모델\n",
    "###  - 엑스트라트리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6149258a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트리의 앙상블 모델 (엑스트라트리)\n",
      "훈련 :  0.7095810143862229\n",
      "테스트 ;  0.7095836606441477\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "etc = ExtraTreesClassifier(n_jobs= -1 , random_state=37)\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "scores = cross_validate(etc,train_input, train_target,return_train_score=True,n_jobs=-1)\n",
    "\n",
    "# 최종 훈련평가 결과 및 검증 결과\n",
    "print('트리의 앙상블 모델 (엑스트라트리)')\n",
    "print('훈련 : ',np.mean(scores['train_score']))\n",
    "print('테스트 ; ',np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "e301cd8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트리의 앙상블 모델(그래디언트 부스팅)\n",
      "훈련 :  0.7095810143862229\n",
      "테스트 :  0.7095836606441477\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gdc = GradientBoostingClassifier(random_state=37)\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "scores = cross_validate(gdc,train_input, train_target,return_train_score=True,n_jobs=-1)\n",
    "\n",
    "# 최종 훈련평가 결과 및 검증 결과\n",
    "print('트리의 앙상블 모델(그래디언트 부스팅)')\n",
    "print('훈련 : ' ,np.mean(scores['train_score']))\n",
    "print('테스트 : ',np.mean(scores['test_score']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee93fc82",
   "metadata": {},
   "source": [
    "## 트리의 앙상블 모델 \n",
    "### - 히스토그램기반 그래디언트 부스팅"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "30efb2d1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "히스토그램기반 그래디언트부스팅(Histogram-base Gradient Boosting)\n",
      "훈련 :  0.7095810143862229\n",
      "테스트 :  0.7095836606441477\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "hgbc = HistGradientBoostingClassifier(random_state=37)\n",
    "scores= cross_validate(hgbc,train_input, train_target,return_train_score=True,n_jobs=-1)\n",
    "print('히스토그램기반 그래디언트부스팅(Histogram-base Gradient Boosting)')\n",
    "print('훈련 : ' ,np.mean(scores['train_score']))\n",
    "print('테스트 : ',np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec744f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## [해석]\n",
    "## 앙상블 모델을 사용했을때 훈련모델과 테스트 모델의 결정계수 차이가 매우 적은 것을 알 수 있습니다\n",
    "## 과대, 과소적합에는 해당되지 않는다.\n",
    "## 훈련 :  0.7095810143862229\n",
    "## 테스트 :  0.7095836606441477\n",
    "## 독립변수 : alone, pclass\n",
    "## 종속변수 : survived \n",
    "## 분류 정확도가 대략 70% 임을 확인할 수 있다 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df298ae7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_kernel",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
