{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25a3a81f",
   "metadata": {},
   "source": [
    "## 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e729e820",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "breast_cancer_data = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "23cefa63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[1.799e+01, 1.038e+01, 1.228e+02, ..., 2.654e-01, 4.601e-01,\n",
       "         1.189e-01],\n",
       "        [2.057e+01, 1.777e+01, 1.329e+02, ..., 1.860e-01, 2.750e-01,\n",
       "         8.902e-02],\n",
       "        [1.969e+01, 2.125e+01, 1.300e+02, ..., 2.430e-01, 3.613e-01,\n",
       "         8.758e-02],\n",
       "        ...,\n",
       "        [1.660e+01, 2.808e+01, 1.083e+02, ..., 1.418e-01, 2.218e-01,\n",
       "         7.820e-02],\n",
       "        [2.060e+01, 2.933e+01, 1.401e+02, ..., 2.650e-01, 4.087e-01,\n",
       "         1.240e-01],\n",
       "        [7.760e+00, 2.454e+01, 4.792e+01, ..., 0.000e+00, 2.871e-01,\n",
       "         7.039e-02]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "        0, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0,\n",
       "        1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "        1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "        0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "        0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "        1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "        1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "        1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "        1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1,\n",
       "        1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['malignant', 'benign'], dtype='<U9'),\n",
       " 'DESCR': '.. _breast_cancer_dataset:\\n\\nBreast cancer wisconsin (diagnostic) dataset\\n--------------------------------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 569\\n\\n    :Number of Attributes: 30 numeric, predictive attributes and the class\\n\\n    :Attribute Information:\\n        - radius (mean of distances from center to points on the perimeter)\\n        - texture (standard deviation of gray-scale values)\\n        - perimeter\\n        - area\\n        - smoothness (local variation in radius lengths)\\n        - compactness (perimeter^2 / area - 1.0)\\n        - concavity (severity of concave portions of the contour)\\n        - concave points (number of concave portions of the contour)\\n        - symmetry\\n        - fractal dimension (\"coastline approximation\" - 1)\\n\\n        The mean, standard error, and \"worst\" or largest (mean of the three\\n        worst/largest values) of these features were computed for each image,\\n        resulting in 30 features.  For instance, field 0 is Mean Radius, field\\n        10 is Radius SE, field 20 is Worst Radius.\\n\\n        - class:\\n                - WDBC-Malignant\\n                - WDBC-Benign\\n\\n    :Summary Statistics:\\n\\n    ===================================== ====== ======\\n                                           Min    Max\\n    ===================================== ====== ======\\n    radius (mean):                        6.981  28.11\\n    texture (mean):                       9.71   39.28\\n    perimeter (mean):                     43.79  188.5\\n    area (mean):                          143.5  2501.0\\n    smoothness (mean):                    0.053  0.163\\n    compactness (mean):                   0.019  0.345\\n    concavity (mean):                     0.0    0.427\\n    concave points (mean):                0.0    0.201\\n    symmetry (mean):                      0.106  0.304\\n    fractal dimension (mean):             0.05   0.097\\n    radius (standard error):              0.112  2.873\\n    texture (standard error):             0.36   4.885\\n    perimeter (standard error):           0.757  21.98\\n    area (standard error):                6.802  542.2\\n    smoothness (standard error):          0.002  0.031\\n    compactness (standard error):         0.002  0.135\\n    concavity (standard error):           0.0    0.396\\n    concave points (standard error):      0.0    0.053\\n    symmetry (standard error):            0.008  0.079\\n    fractal dimension (standard error):   0.001  0.03\\n    radius (worst):                       7.93   36.04\\n    texture (worst):                      12.02  49.54\\n    perimeter (worst):                    50.41  251.2\\n    area (worst):                         185.2  4254.0\\n    smoothness (worst):                   0.071  0.223\\n    compactness (worst):                  0.027  1.058\\n    concavity (worst):                    0.0    1.252\\n    concave points (worst):               0.0    0.291\\n    symmetry (worst):                     0.156  0.664\\n    fractal dimension (worst):            0.055  0.208\\n    ===================================== ====== ======\\n\\n    :Missing Attribute Values: None\\n\\n    :Class Distribution: 212 - Malignant, 357 - Benign\\n\\n    :Creator:  Dr. William H. Wolberg, W. Nick Street, Olvi L. Mangasarian\\n\\n    :Donor: Nick Street\\n\\n    :Date: November, 1995\\n\\nThis is a copy of UCI ML Breast Cancer Wisconsin (Diagnostic) datasets.\\nhttps://goo.gl/U2Uwz2\\n\\nFeatures are computed from a digitized image of a fine needle\\naspirate (FNA) of a breast mass.  They describe\\ncharacteristics of the cell nuclei present in the image.\\n\\nSeparating plane described above was obtained using\\nMultisurface Method-Tree (MSM-T) [K. P. Bennett, \"Decision Tree\\nConstruction Via Linear Programming.\" Proceedings of the 4th\\nMidwest Artificial Intelligence and Cognitive Science Society,\\npp. 97-101, 1992], a classification method which uses linear\\nprogramming to construct a decision tree.  Relevant features\\nwere selected using an exhaustive search in the space of 1-4\\nfeatures and 1-3 separating planes.\\n\\nThe actual linear program used to obtain the separating plane\\nin the 3-dimensional space is that described in:\\n[K. P. Bennett and O. L. Mangasarian: \"Robust Linear\\nProgramming Discrimination of Two Linearly Inseparable Sets\",\\nOptimization Methods and Software 1, 1992, 23-34].\\n\\nThis database is also available through the UW CS ftp server:\\n\\nftp ftp.cs.wisc.edu\\ncd math-prog/cpo-dataset/machine-learn/WDBC/\\n\\n.. topic:: References\\n\\n   - W.N. Street, W.H. Wolberg and O.L. Mangasarian. Nuclear feature extraction \\n     for breast tumor diagnosis. IS&T/SPIE 1993 International Symposium on \\n     Electronic Imaging: Science and Technology, volume 1905, pages 861-870,\\n     San Jose, CA, 1993.\\n   - O.L. Mangasarian, W.N. Street and W.H. Wolberg. Breast cancer diagnosis and \\n     prognosis via linear programming. Operations Research, 43(4), pages 570-577, \\n     July-August 1995.\\n   - W.H. Wolberg, W.N. Street, and O.L. Mangasarian. Machine learning techniques\\n     to diagnose breast cancer from fine-needle aspirates. Cancer Letters 77 (1994) \\n     163-171.',\n",
       " 'feature_names': array(['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
       "        'mean smoothness', 'mean compactness', 'mean concavity',\n",
       "        'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
       "        'radius error', 'texture error', 'perimeter error', 'area error',\n",
       "        'smoothness error', 'compactness error', 'concavity error',\n",
       "        'concave points error', 'symmetry error',\n",
       "        'fractal dimension error', 'worst radius', 'worst texture',\n",
       "        'worst perimeter', 'worst area', 'worst smoothness',\n",
       "        'worst compactness', 'worst concavity', 'worst concave points',\n",
       "        'worst symmetry', 'worst fractal dimension'], dtype='<U23'),\n",
       " 'filename': 'breast_cancer.csv',\n",
       " 'data_module': 'sklearn.datasets.data'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "breast_cancer_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9602ebbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename', 'data_module'])\n"
     ]
    }
   ],
   "source": [
    "print(breast_cancer_data.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4030d74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = breast_cancer_data['data']\n",
    "target = breast_cancer_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "02cd57df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5fd44eb0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.16220</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.12380</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.14440</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.20980</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.13740</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>21.56</td>\n",
       "      <td>22.39</td>\n",
       "      <td>142.00</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.11100</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>0.1726</td>\n",
       "      <td>0.05623</td>\n",
       "      <td>...</td>\n",
       "      <td>26.40</td>\n",
       "      <td>166.10</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>0.14100</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.2216</td>\n",
       "      <td>0.2060</td>\n",
       "      <td>0.07115</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>565</th>\n",
       "      <td>20.13</td>\n",
       "      <td>28.25</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.09780</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>0.1752</td>\n",
       "      <td>0.05533</td>\n",
       "      <td>...</td>\n",
       "      <td>38.25</td>\n",
       "      <td>155.00</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>0.11660</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.1628</td>\n",
       "      <td>0.2572</td>\n",
       "      <td>0.06637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>16.60</td>\n",
       "      <td>28.08</td>\n",
       "      <td>108.30</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.08455</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>0.1590</td>\n",
       "      <td>0.05648</td>\n",
       "      <td>...</td>\n",
       "      <td>34.12</td>\n",
       "      <td>126.70</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>0.11390</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.1418</td>\n",
       "      <td>0.2218</td>\n",
       "      <td>0.07820</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>567</th>\n",
       "      <td>20.60</td>\n",
       "      <td>29.33</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.11780</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>0.2397</td>\n",
       "      <td>0.07016</td>\n",
       "      <td>...</td>\n",
       "      <td>39.42</td>\n",
       "      <td>184.60</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>0.16500</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.2650</td>\n",
       "      <td>0.4087</td>\n",
       "      <td>0.12400</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>568</th>\n",
       "      <td>7.76</td>\n",
       "      <td>24.54</td>\n",
       "      <td>47.92</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.05263</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.1587</td>\n",
       "      <td>0.05884</td>\n",
       "      <td>...</td>\n",
       "      <td>30.37</td>\n",
       "      <td>59.16</td>\n",
       "      <td>268.6</td>\n",
       "      <td>0.08996</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.2871</td>\n",
       "      <td>0.07039</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0          17.99         10.38          122.80     1001.0          0.11840   \n",
       "1          20.57         17.77          132.90     1326.0          0.08474   \n",
       "2          19.69         21.25          130.00     1203.0          0.10960   \n",
       "3          11.42         20.38           77.58      386.1          0.14250   \n",
       "4          20.29         14.34          135.10     1297.0          0.10030   \n",
       "..           ...           ...             ...        ...              ...   \n",
       "564        21.56         22.39          142.00     1479.0          0.11100   \n",
       "565        20.13         28.25          131.20     1261.0          0.09780   \n",
       "566        16.60         28.08          108.30      858.1          0.08455   \n",
       "567        20.60         29.33          140.10     1265.0          0.11780   \n",
       "568         7.76         24.54           47.92      181.0          0.05263   \n",
       "\n",
       "     mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0             0.27760         0.30010              0.14710         0.2419   \n",
       "1             0.07864         0.08690              0.07017         0.1812   \n",
       "2             0.15990         0.19740              0.12790         0.2069   \n",
       "3             0.28390         0.24140              0.10520         0.2597   \n",
       "4             0.13280         0.19800              0.10430         0.1809   \n",
       "..                ...             ...                  ...            ...   \n",
       "564           0.11590         0.24390              0.13890         0.1726   \n",
       "565           0.10340         0.14400              0.09791         0.1752   \n",
       "566           0.10230         0.09251              0.05302         0.1590   \n",
       "567           0.27700         0.35140              0.15200         0.2397   \n",
       "568           0.04362         0.00000              0.00000         0.1587   \n",
       "\n",
       "     mean fractal dimension  ...  worst texture  worst perimeter  worst area  \\\n",
       "0                   0.07871  ...          17.33           184.60      2019.0   \n",
       "1                   0.05667  ...          23.41           158.80      1956.0   \n",
       "2                   0.05999  ...          25.53           152.50      1709.0   \n",
       "3                   0.09744  ...          26.50            98.87       567.7   \n",
       "4                   0.05883  ...          16.67           152.20      1575.0   \n",
       "..                      ...  ...            ...              ...         ...   \n",
       "564                 0.05623  ...          26.40           166.10      2027.0   \n",
       "565                 0.05533  ...          38.25           155.00      1731.0   \n",
       "566                 0.05648  ...          34.12           126.70      1124.0   \n",
       "567                 0.07016  ...          39.42           184.60      1821.0   \n",
       "568                 0.05884  ...          30.37            59.16       268.6   \n",
       "\n",
       "     worst smoothness  worst compactness  worst concavity  \\\n",
       "0             0.16220            0.66560           0.7119   \n",
       "1             0.12380            0.18660           0.2416   \n",
       "2             0.14440            0.42450           0.4504   \n",
       "3             0.20980            0.86630           0.6869   \n",
       "4             0.13740            0.20500           0.4000   \n",
       "..                ...                ...              ...   \n",
       "564           0.14100            0.21130           0.4107   \n",
       "565           0.11660            0.19220           0.3215   \n",
       "566           0.11390            0.30940           0.3403   \n",
       "567           0.16500            0.86810           0.9387   \n",
       "568           0.08996            0.06444           0.0000   \n",
       "\n",
       "     worst concave points  worst symmetry  worst fractal dimension  target  \n",
       "0                  0.2654          0.4601                  0.11890       0  \n",
       "1                  0.1860          0.2750                  0.08902       0  \n",
       "2                  0.2430          0.3613                  0.08758       0  \n",
       "3                  0.2575          0.6638                  0.17300       0  \n",
       "4                  0.1625          0.2364                  0.07678       0  \n",
       "..                    ...             ...                      ...     ...  \n",
       "564                0.2216          0.2060                  0.07115       0  \n",
       "565                0.1628          0.2572                  0.06637       0  \n",
       "566                0.1418          0.2218                  0.07820       0  \n",
       "567                0.2650          0.4087                  0.12400       0  \n",
       "568                0.0000          0.2871                  0.07039       1  \n",
       "\n",
       "[569 rows x 31 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = pd.DataFrame(data,columns=['mean radius', 'mean texture', 'mean perimeter', 'mean area',\n",
    "        'mean smoothness', 'mean compactness', 'mean concavity',\n",
    "        'mean concave points', 'mean symmetry', 'mean fractal dimension',\n",
    "        'radius error', 'texture error', 'perimeter error', 'area error',\n",
    "        'smoothness error', 'compactness error', 'concavity error',\n",
    "        'concave points error', 'symmetry error',\n",
    "        'fractal dimension error', 'worst radius', 'worst texture',\n",
    "        'worst perimeter', 'worst area', 'worst smoothness',\n",
    "        'worst compactness', 'worst concavity', 'worst concave points',\n",
    "        'worst symmetry', 'worst fractal dimension'])\n",
    "data_df['target'] = target\n",
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba6a1a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean radius</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.323782</td>\n",
       "      <td>0.997855</td>\n",
       "      <td>0.987357</td>\n",
       "      <td>0.170581</td>\n",
       "      <td>0.506124</td>\n",
       "      <td>0.676764</td>\n",
       "      <td>0.822529</td>\n",
       "      <td>0.147741</td>\n",
       "      <td>-0.311631</td>\n",
       "      <td>...</td>\n",
       "      <td>0.297008</td>\n",
       "      <td>0.965137</td>\n",
       "      <td>0.941082</td>\n",
       "      <td>0.119616</td>\n",
       "      <td>0.413463</td>\n",
       "      <td>0.526911</td>\n",
       "      <td>0.744214</td>\n",
       "      <td>0.163953</td>\n",
       "      <td>0.007066</td>\n",
       "      <td>-0.730029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean texture</th>\n",
       "      <td>0.323782</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.329533</td>\n",
       "      <td>0.321086</td>\n",
       "      <td>-0.023389</td>\n",
       "      <td>0.236702</td>\n",
       "      <td>0.302418</td>\n",
       "      <td>0.293464</td>\n",
       "      <td>0.071401</td>\n",
       "      <td>-0.076437</td>\n",
       "      <td>...</td>\n",
       "      <td>0.912045</td>\n",
       "      <td>0.358040</td>\n",
       "      <td>0.343546</td>\n",
       "      <td>0.077503</td>\n",
       "      <td>0.277830</td>\n",
       "      <td>0.301025</td>\n",
       "      <td>0.295316</td>\n",
       "      <td>0.105008</td>\n",
       "      <td>0.119205</td>\n",
       "      <td>-0.415185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean perimeter</th>\n",
       "      <td>0.997855</td>\n",
       "      <td>0.329533</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.986507</td>\n",
       "      <td>0.207278</td>\n",
       "      <td>0.556936</td>\n",
       "      <td>0.716136</td>\n",
       "      <td>0.850977</td>\n",
       "      <td>0.183027</td>\n",
       "      <td>-0.261477</td>\n",
       "      <td>...</td>\n",
       "      <td>0.303038</td>\n",
       "      <td>0.970387</td>\n",
       "      <td>0.941550</td>\n",
       "      <td>0.150549</td>\n",
       "      <td>0.455774</td>\n",
       "      <td>0.563879</td>\n",
       "      <td>0.771241</td>\n",
       "      <td>0.189115</td>\n",
       "      <td>0.051019</td>\n",
       "      <td>-0.742636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean area</th>\n",
       "      <td>0.987357</td>\n",
       "      <td>0.321086</td>\n",
       "      <td>0.986507</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.177028</td>\n",
       "      <td>0.498502</td>\n",
       "      <td>0.685983</td>\n",
       "      <td>0.823269</td>\n",
       "      <td>0.151293</td>\n",
       "      <td>-0.283110</td>\n",
       "      <td>...</td>\n",
       "      <td>0.287489</td>\n",
       "      <td>0.959120</td>\n",
       "      <td>0.959213</td>\n",
       "      <td>0.123523</td>\n",
       "      <td>0.390410</td>\n",
       "      <td>0.512606</td>\n",
       "      <td>0.722017</td>\n",
       "      <td>0.143570</td>\n",
       "      <td>0.003738</td>\n",
       "      <td>-0.708984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean smoothness</th>\n",
       "      <td>0.170581</td>\n",
       "      <td>-0.023389</td>\n",
       "      <td>0.207278</td>\n",
       "      <td>0.177028</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.659123</td>\n",
       "      <td>0.521984</td>\n",
       "      <td>0.553695</td>\n",
       "      <td>0.557775</td>\n",
       "      <td>0.584792</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036072</td>\n",
       "      <td>0.238853</td>\n",
       "      <td>0.206718</td>\n",
       "      <td>0.805324</td>\n",
       "      <td>0.472468</td>\n",
       "      <td>0.434926</td>\n",
       "      <td>0.503053</td>\n",
       "      <td>0.394309</td>\n",
       "      <td>0.499316</td>\n",
       "      <td>-0.358560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean compactness</th>\n",
       "      <td>0.506124</td>\n",
       "      <td>0.236702</td>\n",
       "      <td>0.556936</td>\n",
       "      <td>0.498502</td>\n",
       "      <td>0.659123</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.883121</td>\n",
       "      <td>0.831135</td>\n",
       "      <td>0.602641</td>\n",
       "      <td>0.565369</td>\n",
       "      <td>...</td>\n",
       "      <td>0.248133</td>\n",
       "      <td>0.590210</td>\n",
       "      <td>0.509604</td>\n",
       "      <td>0.565541</td>\n",
       "      <td>0.865809</td>\n",
       "      <td>0.816275</td>\n",
       "      <td>0.815573</td>\n",
       "      <td>0.510223</td>\n",
       "      <td>0.687382</td>\n",
       "      <td>-0.596534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean concavity</th>\n",
       "      <td>0.676764</td>\n",
       "      <td>0.302418</td>\n",
       "      <td>0.716136</td>\n",
       "      <td>0.685983</td>\n",
       "      <td>0.521984</td>\n",
       "      <td>0.883121</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.921391</td>\n",
       "      <td>0.500667</td>\n",
       "      <td>0.336783</td>\n",
       "      <td>...</td>\n",
       "      <td>0.299879</td>\n",
       "      <td>0.729565</td>\n",
       "      <td>0.675987</td>\n",
       "      <td>0.448822</td>\n",
       "      <td>0.754968</td>\n",
       "      <td>0.884103</td>\n",
       "      <td>0.861323</td>\n",
       "      <td>0.409464</td>\n",
       "      <td>0.514930</td>\n",
       "      <td>-0.696360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean concave points</th>\n",
       "      <td>0.822529</td>\n",
       "      <td>0.293464</td>\n",
       "      <td>0.850977</td>\n",
       "      <td>0.823269</td>\n",
       "      <td>0.553695</td>\n",
       "      <td>0.831135</td>\n",
       "      <td>0.921391</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.462497</td>\n",
       "      <td>0.166917</td>\n",
       "      <td>...</td>\n",
       "      <td>0.292752</td>\n",
       "      <td>0.855923</td>\n",
       "      <td>0.809630</td>\n",
       "      <td>0.452753</td>\n",
       "      <td>0.667454</td>\n",
       "      <td>0.752399</td>\n",
       "      <td>0.910155</td>\n",
       "      <td>0.375744</td>\n",
       "      <td>0.368661</td>\n",
       "      <td>-0.776614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean symmetry</th>\n",
       "      <td>0.147741</td>\n",
       "      <td>0.071401</td>\n",
       "      <td>0.183027</td>\n",
       "      <td>0.151293</td>\n",
       "      <td>0.557775</td>\n",
       "      <td>0.602641</td>\n",
       "      <td>0.500667</td>\n",
       "      <td>0.462497</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.479921</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090651</td>\n",
       "      <td>0.219169</td>\n",
       "      <td>0.177193</td>\n",
       "      <td>0.426675</td>\n",
       "      <td>0.473200</td>\n",
       "      <td>0.433721</td>\n",
       "      <td>0.430297</td>\n",
       "      <td>0.699826</td>\n",
       "      <td>0.438413</td>\n",
       "      <td>-0.330499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <td>-0.311631</td>\n",
       "      <td>-0.076437</td>\n",
       "      <td>-0.261477</td>\n",
       "      <td>-0.283110</td>\n",
       "      <td>0.584792</td>\n",
       "      <td>0.565369</td>\n",
       "      <td>0.336783</td>\n",
       "      <td>0.166917</td>\n",
       "      <td>0.479921</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.051269</td>\n",
       "      <td>-0.205151</td>\n",
       "      <td>-0.231854</td>\n",
       "      <td>0.504942</td>\n",
       "      <td>0.458798</td>\n",
       "      <td>0.346234</td>\n",
       "      <td>0.175325</td>\n",
       "      <td>0.334019</td>\n",
       "      <td>0.767297</td>\n",
       "      <td>0.012838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>radius error</th>\n",
       "      <td>0.679090</td>\n",
       "      <td>0.275869</td>\n",
       "      <td>0.691765</td>\n",
       "      <td>0.732562</td>\n",
       "      <td>0.301467</td>\n",
       "      <td>0.497473</td>\n",
       "      <td>0.631925</td>\n",
       "      <td>0.698050</td>\n",
       "      <td>0.303379</td>\n",
       "      <td>0.000111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.194799</td>\n",
       "      <td>0.719684</td>\n",
       "      <td>0.751548</td>\n",
       "      <td>0.141919</td>\n",
       "      <td>0.287103</td>\n",
       "      <td>0.380585</td>\n",
       "      <td>0.531062</td>\n",
       "      <td>0.094543</td>\n",
       "      <td>0.049559</td>\n",
       "      <td>-0.567134</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>texture error</th>\n",
       "      <td>-0.097317</td>\n",
       "      <td>0.386358</td>\n",
       "      <td>-0.086761</td>\n",
       "      <td>-0.066280</td>\n",
       "      <td>0.068406</td>\n",
       "      <td>0.046205</td>\n",
       "      <td>0.076218</td>\n",
       "      <td>0.021480</td>\n",
       "      <td>0.128053</td>\n",
       "      <td>0.164174</td>\n",
       "      <td>...</td>\n",
       "      <td>0.409003</td>\n",
       "      <td>-0.102242</td>\n",
       "      <td>-0.083195</td>\n",
       "      <td>-0.073658</td>\n",
       "      <td>-0.092439</td>\n",
       "      <td>-0.068956</td>\n",
       "      <td>-0.119638</td>\n",
       "      <td>-0.128215</td>\n",
       "      <td>-0.045655</td>\n",
       "      <td>0.008303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>perimeter error</th>\n",
       "      <td>0.674172</td>\n",
       "      <td>0.281673</td>\n",
       "      <td>0.693135</td>\n",
       "      <td>0.726628</td>\n",
       "      <td>0.296092</td>\n",
       "      <td>0.548905</td>\n",
       "      <td>0.660391</td>\n",
       "      <td>0.710650</td>\n",
       "      <td>0.313893</td>\n",
       "      <td>0.039830</td>\n",
       "      <td>...</td>\n",
       "      <td>0.200371</td>\n",
       "      <td>0.721031</td>\n",
       "      <td>0.730713</td>\n",
       "      <td>0.130054</td>\n",
       "      <td>0.341919</td>\n",
       "      <td>0.418899</td>\n",
       "      <td>0.554897</td>\n",
       "      <td>0.109930</td>\n",
       "      <td>0.085433</td>\n",
       "      <td>-0.556141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>area error</th>\n",
       "      <td>0.735864</td>\n",
       "      <td>0.259845</td>\n",
       "      <td>0.744983</td>\n",
       "      <td>0.800086</td>\n",
       "      <td>0.246552</td>\n",
       "      <td>0.455653</td>\n",
       "      <td>0.617427</td>\n",
       "      <td>0.690299</td>\n",
       "      <td>0.223970</td>\n",
       "      <td>-0.090170</td>\n",
       "      <td>...</td>\n",
       "      <td>0.196497</td>\n",
       "      <td>0.761213</td>\n",
       "      <td>0.811408</td>\n",
       "      <td>0.125389</td>\n",
       "      <td>0.283257</td>\n",
       "      <td>0.385100</td>\n",
       "      <td>0.538166</td>\n",
       "      <td>0.074126</td>\n",
       "      <td>0.017539</td>\n",
       "      <td>-0.548236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoothness error</th>\n",
       "      <td>-0.222600</td>\n",
       "      <td>0.006614</td>\n",
       "      <td>-0.202694</td>\n",
       "      <td>-0.166777</td>\n",
       "      <td>0.332375</td>\n",
       "      <td>0.135299</td>\n",
       "      <td>0.098564</td>\n",
       "      <td>0.027653</td>\n",
       "      <td>0.187321</td>\n",
       "      <td>0.401964</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.074743</td>\n",
       "      <td>-0.217304</td>\n",
       "      <td>-0.182195</td>\n",
       "      <td>0.314457</td>\n",
       "      <td>-0.055558</td>\n",
       "      <td>-0.058298</td>\n",
       "      <td>-0.102007</td>\n",
       "      <td>-0.107342</td>\n",
       "      <td>0.101480</td>\n",
       "      <td>0.067016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>compactness error</th>\n",
       "      <td>0.206000</td>\n",
       "      <td>0.191975</td>\n",
       "      <td>0.250744</td>\n",
       "      <td>0.212583</td>\n",
       "      <td>0.318943</td>\n",
       "      <td>0.738722</td>\n",
       "      <td>0.670279</td>\n",
       "      <td>0.490424</td>\n",
       "      <td>0.421659</td>\n",
       "      <td>0.559837</td>\n",
       "      <td>...</td>\n",
       "      <td>0.143003</td>\n",
       "      <td>0.260516</td>\n",
       "      <td>0.199371</td>\n",
       "      <td>0.227394</td>\n",
       "      <td>0.678780</td>\n",
       "      <td>0.639147</td>\n",
       "      <td>0.483208</td>\n",
       "      <td>0.277878</td>\n",
       "      <td>0.590973</td>\n",
       "      <td>-0.292999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concavity error</th>\n",
       "      <td>0.194204</td>\n",
       "      <td>0.143293</td>\n",
       "      <td>0.228082</td>\n",
       "      <td>0.207660</td>\n",
       "      <td>0.248396</td>\n",
       "      <td>0.570517</td>\n",
       "      <td>0.691270</td>\n",
       "      <td>0.439167</td>\n",
       "      <td>0.342627</td>\n",
       "      <td>0.446630</td>\n",
       "      <td>...</td>\n",
       "      <td>0.100241</td>\n",
       "      <td>0.226680</td>\n",
       "      <td>0.188353</td>\n",
       "      <td>0.168481</td>\n",
       "      <td>0.484858</td>\n",
       "      <td>0.662564</td>\n",
       "      <td>0.440472</td>\n",
       "      <td>0.197788</td>\n",
       "      <td>0.439329</td>\n",
       "      <td>-0.253730</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>concave points error</th>\n",
       "      <td>0.376169</td>\n",
       "      <td>0.163851</td>\n",
       "      <td>0.407217</td>\n",
       "      <td>0.372320</td>\n",
       "      <td>0.380676</td>\n",
       "      <td>0.642262</td>\n",
       "      <td>0.683260</td>\n",
       "      <td>0.615634</td>\n",
       "      <td>0.393298</td>\n",
       "      <td>0.341198</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086741</td>\n",
       "      <td>0.394999</td>\n",
       "      <td>0.342271</td>\n",
       "      <td>0.215351</td>\n",
       "      <td>0.452888</td>\n",
       "      <td>0.549592</td>\n",
       "      <td>0.602450</td>\n",
       "      <td>0.143116</td>\n",
       "      <td>0.310655</td>\n",
       "      <td>-0.408042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>symmetry error</th>\n",
       "      <td>-0.104321</td>\n",
       "      <td>0.009127</td>\n",
       "      <td>-0.081629</td>\n",
       "      <td>-0.072497</td>\n",
       "      <td>0.200774</td>\n",
       "      <td>0.229977</td>\n",
       "      <td>0.178009</td>\n",
       "      <td>0.095351</td>\n",
       "      <td>0.449137</td>\n",
       "      <td>0.345007</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.077473</td>\n",
       "      <td>-0.103753</td>\n",
       "      <td>-0.110343</td>\n",
       "      <td>-0.012662</td>\n",
       "      <td>0.060255</td>\n",
       "      <td>0.037119</td>\n",
       "      <td>-0.030413</td>\n",
       "      <td>0.389402</td>\n",
       "      <td>0.078079</td>\n",
       "      <td>0.006522</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fractal dimension error</th>\n",
       "      <td>-0.042641</td>\n",
       "      <td>0.054458</td>\n",
       "      <td>-0.005523</td>\n",
       "      <td>-0.019887</td>\n",
       "      <td>0.283607</td>\n",
       "      <td>0.507318</td>\n",
       "      <td>0.449301</td>\n",
       "      <td>0.257584</td>\n",
       "      <td>0.331786</td>\n",
       "      <td>0.688132</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.003195</td>\n",
       "      <td>-0.001000</td>\n",
       "      <td>-0.022736</td>\n",
       "      <td>0.170568</td>\n",
       "      <td>0.390159</td>\n",
       "      <td>0.379975</td>\n",
       "      <td>0.215204</td>\n",
       "      <td>0.111094</td>\n",
       "      <td>0.591328</td>\n",
       "      <td>-0.077972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst radius</th>\n",
       "      <td>0.969539</td>\n",
       "      <td>0.352573</td>\n",
       "      <td>0.969476</td>\n",
       "      <td>0.962746</td>\n",
       "      <td>0.213120</td>\n",
       "      <td>0.535315</td>\n",
       "      <td>0.688236</td>\n",
       "      <td>0.830318</td>\n",
       "      <td>0.185728</td>\n",
       "      <td>-0.253691</td>\n",
       "      <td>...</td>\n",
       "      <td>0.359921</td>\n",
       "      <td>0.993708</td>\n",
       "      <td>0.984015</td>\n",
       "      <td>0.216574</td>\n",
       "      <td>0.475820</td>\n",
       "      <td>0.573975</td>\n",
       "      <td>0.787424</td>\n",
       "      <td>0.243529</td>\n",
       "      <td>0.093492</td>\n",
       "      <td>-0.776454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst texture</th>\n",
       "      <td>0.297008</td>\n",
       "      <td>0.912045</td>\n",
       "      <td>0.303038</td>\n",
       "      <td>0.287489</td>\n",
       "      <td>0.036072</td>\n",
       "      <td>0.248133</td>\n",
       "      <td>0.299879</td>\n",
       "      <td>0.292752</td>\n",
       "      <td>0.090651</td>\n",
       "      <td>-0.051269</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.365098</td>\n",
       "      <td>0.345842</td>\n",
       "      <td>0.225429</td>\n",
       "      <td>0.360832</td>\n",
       "      <td>0.368366</td>\n",
       "      <td>0.359755</td>\n",
       "      <td>0.233027</td>\n",
       "      <td>0.219122</td>\n",
       "      <td>-0.456903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst perimeter</th>\n",
       "      <td>0.965137</td>\n",
       "      <td>0.358040</td>\n",
       "      <td>0.970387</td>\n",
       "      <td>0.959120</td>\n",
       "      <td>0.238853</td>\n",
       "      <td>0.590210</td>\n",
       "      <td>0.729565</td>\n",
       "      <td>0.855923</td>\n",
       "      <td>0.219169</td>\n",
       "      <td>-0.205151</td>\n",
       "      <td>...</td>\n",
       "      <td>0.365098</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.977578</td>\n",
       "      <td>0.236775</td>\n",
       "      <td>0.529408</td>\n",
       "      <td>0.618344</td>\n",
       "      <td>0.816322</td>\n",
       "      <td>0.269493</td>\n",
       "      <td>0.138957</td>\n",
       "      <td>-0.782914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst area</th>\n",
       "      <td>0.941082</td>\n",
       "      <td>0.343546</td>\n",
       "      <td>0.941550</td>\n",
       "      <td>0.959213</td>\n",
       "      <td>0.206718</td>\n",
       "      <td>0.509604</td>\n",
       "      <td>0.675987</td>\n",
       "      <td>0.809630</td>\n",
       "      <td>0.177193</td>\n",
       "      <td>-0.231854</td>\n",
       "      <td>...</td>\n",
       "      <td>0.345842</td>\n",
       "      <td>0.977578</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.209145</td>\n",
       "      <td>0.438296</td>\n",
       "      <td>0.543331</td>\n",
       "      <td>0.747419</td>\n",
       "      <td>0.209146</td>\n",
       "      <td>0.079647</td>\n",
       "      <td>-0.733825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst smoothness</th>\n",
       "      <td>0.119616</td>\n",
       "      <td>0.077503</td>\n",
       "      <td>0.150549</td>\n",
       "      <td>0.123523</td>\n",
       "      <td>0.805324</td>\n",
       "      <td>0.565541</td>\n",
       "      <td>0.448822</td>\n",
       "      <td>0.452753</td>\n",
       "      <td>0.426675</td>\n",
       "      <td>0.504942</td>\n",
       "      <td>...</td>\n",
       "      <td>0.225429</td>\n",
       "      <td>0.236775</td>\n",
       "      <td>0.209145</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.568187</td>\n",
       "      <td>0.518523</td>\n",
       "      <td>0.547691</td>\n",
       "      <td>0.493838</td>\n",
       "      <td>0.617624</td>\n",
       "      <td>-0.421465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst compactness</th>\n",
       "      <td>0.413463</td>\n",
       "      <td>0.277830</td>\n",
       "      <td>0.455774</td>\n",
       "      <td>0.390410</td>\n",
       "      <td>0.472468</td>\n",
       "      <td>0.865809</td>\n",
       "      <td>0.754968</td>\n",
       "      <td>0.667454</td>\n",
       "      <td>0.473200</td>\n",
       "      <td>0.458798</td>\n",
       "      <td>...</td>\n",
       "      <td>0.360832</td>\n",
       "      <td>0.529408</td>\n",
       "      <td>0.438296</td>\n",
       "      <td>0.568187</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.892261</td>\n",
       "      <td>0.801080</td>\n",
       "      <td>0.614441</td>\n",
       "      <td>0.810455</td>\n",
       "      <td>-0.590998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst concavity</th>\n",
       "      <td>0.526911</td>\n",
       "      <td>0.301025</td>\n",
       "      <td>0.563879</td>\n",
       "      <td>0.512606</td>\n",
       "      <td>0.434926</td>\n",
       "      <td>0.816275</td>\n",
       "      <td>0.884103</td>\n",
       "      <td>0.752399</td>\n",
       "      <td>0.433721</td>\n",
       "      <td>0.346234</td>\n",
       "      <td>...</td>\n",
       "      <td>0.368366</td>\n",
       "      <td>0.618344</td>\n",
       "      <td>0.543331</td>\n",
       "      <td>0.518523</td>\n",
       "      <td>0.892261</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.855434</td>\n",
       "      <td>0.532520</td>\n",
       "      <td>0.686511</td>\n",
       "      <td>-0.659610</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst concave points</th>\n",
       "      <td>0.744214</td>\n",
       "      <td>0.295316</td>\n",
       "      <td>0.771241</td>\n",
       "      <td>0.722017</td>\n",
       "      <td>0.503053</td>\n",
       "      <td>0.815573</td>\n",
       "      <td>0.861323</td>\n",
       "      <td>0.910155</td>\n",
       "      <td>0.430297</td>\n",
       "      <td>0.175325</td>\n",
       "      <td>...</td>\n",
       "      <td>0.359755</td>\n",
       "      <td>0.816322</td>\n",
       "      <td>0.747419</td>\n",
       "      <td>0.547691</td>\n",
       "      <td>0.801080</td>\n",
       "      <td>0.855434</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.502528</td>\n",
       "      <td>0.511114</td>\n",
       "      <td>-0.793566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst symmetry</th>\n",
       "      <td>0.163953</td>\n",
       "      <td>0.105008</td>\n",
       "      <td>0.189115</td>\n",
       "      <td>0.143570</td>\n",
       "      <td>0.394309</td>\n",
       "      <td>0.510223</td>\n",
       "      <td>0.409464</td>\n",
       "      <td>0.375744</td>\n",
       "      <td>0.699826</td>\n",
       "      <td>0.334019</td>\n",
       "      <td>...</td>\n",
       "      <td>0.233027</td>\n",
       "      <td>0.269493</td>\n",
       "      <td>0.209146</td>\n",
       "      <td>0.493838</td>\n",
       "      <td>0.614441</td>\n",
       "      <td>0.532520</td>\n",
       "      <td>0.502528</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.537848</td>\n",
       "      <td>-0.416294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>worst fractal dimension</th>\n",
       "      <td>0.007066</td>\n",
       "      <td>0.119205</td>\n",
       "      <td>0.051019</td>\n",
       "      <td>0.003738</td>\n",
       "      <td>0.499316</td>\n",
       "      <td>0.687382</td>\n",
       "      <td>0.514930</td>\n",
       "      <td>0.368661</td>\n",
       "      <td>0.438413</td>\n",
       "      <td>0.767297</td>\n",
       "      <td>...</td>\n",
       "      <td>0.219122</td>\n",
       "      <td>0.138957</td>\n",
       "      <td>0.079647</td>\n",
       "      <td>0.617624</td>\n",
       "      <td>0.810455</td>\n",
       "      <td>0.686511</td>\n",
       "      <td>0.511114</td>\n",
       "      <td>0.537848</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.323872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>target</th>\n",
       "      <td>-0.730029</td>\n",
       "      <td>-0.415185</td>\n",
       "      <td>-0.742636</td>\n",
       "      <td>-0.708984</td>\n",
       "      <td>-0.358560</td>\n",
       "      <td>-0.596534</td>\n",
       "      <td>-0.696360</td>\n",
       "      <td>-0.776614</td>\n",
       "      <td>-0.330499</td>\n",
       "      <td>0.012838</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.456903</td>\n",
       "      <td>-0.782914</td>\n",
       "      <td>-0.733825</td>\n",
       "      <td>-0.421465</td>\n",
       "      <td>-0.590998</td>\n",
       "      <td>-0.659610</td>\n",
       "      <td>-0.793566</td>\n",
       "      <td>-0.416294</td>\n",
       "      <td>-0.323872</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31 rows × 31 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         mean radius  mean texture  mean perimeter  mean area  \\\n",
       "mean radius                 1.000000      0.323782        0.997855   0.987357   \n",
       "mean texture                0.323782      1.000000        0.329533   0.321086   \n",
       "mean perimeter              0.997855      0.329533        1.000000   0.986507   \n",
       "mean area                   0.987357      0.321086        0.986507   1.000000   \n",
       "mean smoothness             0.170581     -0.023389        0.207278   0.177028   \n",
       "mean compactness            0.506124      0.236702        0.556936   0.498502   \n",
       "mean concavity              0.676764      0.302418        0.716136   0.685983   \n",
       "mean concave points         0.822529      0.293464        0.850977   0.823269   \n",
       "mean symmetry               0.147741      0.071401        0.183027   0.151293   \n",
       "mean fractal dimension     -0.311631     -0.076437       -0.261477  -0.283110   \n",
       "radius error                0.679090      0.275869        0.691765   0.732562   \n",
       "texture error              -0.097317      0.386358       -0.086761  -0.066280   \n",
       "perimeter error             0.674172      0.281673        0.693135   0.726628   \n",
       "area error                  0.735864      0.259845        0.744983   0.800086   \n",
       "smoothness error           -0.222600      0.006614       -0.202694  -0.166777   \n",
       "compactness error           0.206000      0.191975        0.250744   0.212583   \n",
       "concavity error             0.194204      0.143293        0.228082   0.207660   \n",
       "concave points error        0.376169      0.163851        0.407217   0.372320   \n",
       "symmetry error             -0.104321      0.009127       -0.081629  -0.072497   \n",
       "fractal dimension error    -0.042641      0.054458       -0.005523  -0.019887   \n",
       "worst radius                0.969539      0.352573        0.969476   0.962746   \n",
       "worst texture               0.297008      0.912045        0.303038   0.287489   \n",
       "worst perimeter             0.965137      0.358040        0.970387   0.959120   \n",
       "worst area                  0.941082      0.343546        0.941550   0.959213   \n",
       "worst smoothness            0.119616      0.077503        0.150549   0.123523   \n",
       "worst compactness           0.413463      0.277830        0.455774   0.390410   \n",
       "worst concavity             0.526911      0.301025        0.563879   0.512606   \n",
       "worst concave points        0.744214      0.295316        0.771241   0.722017   \n",
       "worst symmetry              0.163953      0.105008        0.189115   0.143570   \n",
       "worst fractal dimension     0.007066      0.119205        0.051019   0.003738   \n",
       "target                     -0.730029     -0.415185       -0.742636  -0.708984   \n",
       "\n",
       "                         mean smoothness  mean compactness  mean concavity  \\\n",
       "mean radius                     0.170581          0.506124        0.676764   \n",
       "mean texture                   -0.023389          0.236702        0.302418   \n",
       "mean perimeter                  0.207278          0.556936        0.716136   \n",
       "mean area                       0.177028          0.498502        0.685983   \n",
       "mean smoothness                 1.000000          0.659123        0.521984   \n",
       "mean compactness                0.659123          1.000000        0.883121   \n",
       "mean concavity                  0.521984          0.883121        1.000000   \n",
       "mean concave points             0.553695          0.831135        0.921391   \n",
       "mean symmetry                   0.557775          0.602641        0.500667   \n",
       "mean fractal dimension          0.584792          0.565369        0.336783   \n",
       "radius error                    0.301467          0.497473        0.631925   \n",
       "texture error                   0.068406          0.046205        0.076218   \n",
       "perimeter error                 0.296092          0.548905        0.660391   \n",
       "area error                      0.246552          0.455653        0.617427   \n",
       "smoothness error                0.332375          0.135299        0.098564   \n",
       "compactness error               0.318943          0.738722        0.670279   \n",
       "concavity error                 0.248396          0.570517        0.691270   \n",
       "concave points error            0.380676          0.642262        0.683260   \n",
       "symmetry error                  0.200774          0.229977        0.178009   \n",
       "fractal dimension error         0.283607          0.507318        0.449301   \n",
       "worst radius                    0.213120          0.535315        0.688236   \n",
       "worst texture                   0.036072          0.248133        0.299879   \n",
       "worst perimeter                 0.238853          0.590210        0.729565   \n",
       "worst area                      0.206718          0.509604        0.675987   \n",
       "worst smoothness                0.805324          0.565541        0.448822   \n",
       "worst compactness               0.472468          0.865809        0.754968   \n",
       "worst concavity                 0.434926          0.816275        0.884103   \n",
       "worst concave points            0.503053          0.815573        0.861323   \n",
       "worst symmetry                  0.394309          0.510223        0.409464   \n",
       "worst fractal dimension         0.499316          0.687382        0.514930   \n",
       "target                         -0.358560         -0.596534       -0.696360   \n",
       "\n",
       "                         mean concave points  mean symmetry  \\\n",
       "mean radius                         0.822529       0.147741   \n",
       "mean texture                        0.293464       0.071401   \n",
       "mean perimeter                      0.850977       0.183027   \n",
       "mean area                           0.823269       0.151293   \n",
       "mean smoothness                     0.553695       0.557775   \n",
       "mean compactness                    0.831135       0.602641   \n",
       "mean concavity                      0.921391       0.500667   \n",
       "mean concave points                 1.000000       0.462497   \n",
       "mean symmetry                       0.462497       1.000000   \n",
       "mean fractal dimension              0.166917       0.479921   \n",
       "radius error                        0.698050       0.303379   \n",
       "texture error                       0.021480       0.128053   \n",
       "perimeter error                     0.710650       0.313893   \n",
       "area error                          0.690299       0.223970   \n",
       "smoothness error                    0.027653       0.187321   \n",
       "compactness error                   0.490424       0.421659   \n",
       "concavity error                     0.439167       0.342627   \n",
       "concave points error                0.615634       0.393298   \n",
       "symmetry error                      0.095351       0.449137   \n",
       "fractal dimension error             0.257584       0.331786   \n",
       "worst radius                        0.830318       0.185728   \n",
       "worst texture                       0.292752       0.090651   \n",
       "worst perimeter                     0.855923       0.219169   \n",
       "worst area                          0.809630       0.177193   \n",
       "worst smoothness                    0.452753       0.426675   \n",
       "worst compactness                   0.667454       0.473200   \n",
       "worst concavity                     0.752399       0.433721   \n",
       "worst concave points                0.910155       0.430297   \n",
       "worst symmetry                      0.375744       0.699826   \n",
       "worst fractal dimension             0.368661       0.438413   \n",
       "target                             -0.776614      -0.330499   \n",
       "\n",
       "                         mean fractal dimension  ...  worst texture  \\\n",
       "mean radius                           -0.311631  ...       0.297008   \n",
       "mean texture                          -0.076437  ...       0.912045   \n",
       "mean perimeter                        -0.261477  ...       0.303038   \n",
       "mean area                             -0.283110  ...       0.287489   \n",
       "mean smoothness                        0.584792  ...       0.036072   \n",
       "mean compactness                       0.565369  ...       0.248133   \n",
       "mean concavity                         0.336783  ...       0.299879   \n",
       "mean concave points                    0.166917  ...       0.292752   \n",
       "mean symmetry                          0.479921  ...       0.090651   \n",
       "mean fractal dimension                 1.000000  ...      -0.051269   \n",
       "radius error                           0.000111  ...       0.194799   \n",
       "texture error                          0.164174  ...       0.409003   \n",
       "perimeter error                        0.039830  ...       0.200371   \n",
       "area error                            -0.090170  ...       0.196497   \n",
       "smoothness error                       0.401964  ...      -0.074743   \n",
       "compactness error                      0.559837  ...       0.143003   \n",
       "concavity error                        0.446630  ...       0.100241   \n",
       "concave points error                   0.341198  ...       0.086741   \n",
       "symmetry error                         0.345007  ...      -0.077473   \n",
       "fractal dimension error                0.688132  ...      -0.003195   \n",
       "worst radius                          -0.253691  ...       0.359921   \n",
       "worst texture                         -0.051269  ...       1.000000   \n",
       "worst perimeter                       -0.205151  ...       0.365098   \n",
       "worst area                            -0.231854  ...       0.345842   \n",
       "worst smoothness                       0.504942  ...       0.225429   \n",
       "worst compactness                      0.458798  ...       0.360832   \n",
       "worst concavity                        0.346234  ...       0.368366   \n",
       "worst concave points                   0.175325  ...       0.359755   \n",
       "worst symmetry                         0.334019  ...       0.233027   \n",
       "worst fractal dimension                0.767297  ...       0.219122   \n",
       "target                                 0.012838  ...      -0.456903   \n",
       "\n",
       "                         worst perimeter  worst area  worst smoothness  \\\n",
       "mean radius                     0.965137    0.941082          0.119616   \n",
       "mean texture                    0.358040    0.343546          0.077503   \n",
       "mean perimeter                  0.970387    0.941550          0.150549   \n",
       "mean area                       0.959120    0.959213          0.123523   \n",
       "mean smoothness                 0.238853    0.206718          0.805324   \n",
       "mean compactness                0.590210    0.509604          0.565541   \n",
       "mean concavity                  0.729565    0.675987          0.448822   \n",
       "mean concave points             0.855923    0.809630          0.452753   \n",
       "mean symmetry                   0.219169    0.177193          0.426675   \n",
       "mean fractal dimension         -0.205151   -0.231854          0.504942   \n",
       "radius error                    0.719684    0.751548          0.141919   \n",
       "texture error                  -0.102242   -0.083195         -0.073658   \n",
       "perimeter error                 0.721031    0.730713          0.130054   \n",
       "area error                      0.761213    0.811408          0.125389   \n",
       "smoothness error               -0.217304   -0.182195          0.314457   \n",
       "compactness error               0.260516    0.199371          0.227394   \n",
       "concavity error                 0.226680    0.188353          0.168481   \n",
       "concave points error            0.394999    0.342271          0.215351   \n",
       "symmetry error                 -0.103753   -0.110343         -0.012662   \n",
       "fractal dimension error        -0.001000   -0.022736          0.170568   \n",
       "worst radius                    0.993708    0.984015          0.216574   \n",
       "worst texture                   0.365098    0.345842          0.225429   \n",
       "worst perimeter                 1.000000    0.977578          0.236775   \n",
       "worst area                      0.977578    1.000000          0.209145   \n",
       "worst smoothness                0.236775    0.209145          1.000000   \n",
       "worst compactness               0.529408    0.438296          0.568187   \n",
       "worst concavity                 0.618344    0.543331          0.518523   \n",
       "worst concave points            0.816322    0.747419          0.547691   \n",
       "worst symmetry                  0.269493    0.209146          0.493838   \n",
       "worst fractal dimension         0.138957    0.079647          0.617624   \n",
       "target                         -0.782914   -0.733825         -0.421465   \n",
       "\n",
       "                         worst compactness  worst concavity  \\\n",
       "mean radius                       0.413463         0.526911   \n",
       "mean texture                      0.277830         0.301025   \n",
       "mean perimeter                    0.455774         0.563879   \n",
       "mean area                         0.390410         0.512606   \n",
       "mean smoothness                   0.472468         0.434926   \n",
       "mean compactness                  0.865809         0.816275   \n",
       "mean concavity                    0.754968         0.884103   \n",
       "mean concave points               0.667454         0.752399   \n",
       "mean symmetry                     0.473200         0.433721   \n",
       "mean fractal dimension            0.458798         0.346234   \n",
       "radius error                      0.287103         0.380585   \n",
       "texture error                    -0.092439        -0.068956   \n",
       "perimeter error                   0.341919         0.418899   \n",
       "area error                        0.283257         0.385100   \n",
       "smoothness error                 -0.055558        -0.058298   \n",
       "compactness error                 0.678780         0.639147   \n",
       "concavity error                   0.484858         0.662564   \n",
       "concave points error              0.452888         0.549592   \n",
       "symmetry error                    0.060255         0.037119   \n",
       "fractal dimension error           0.390159         0.379975   \n",
       "worst radius                      0.475820         0.573975   \n",
       "worst texture                     0.360832         0.368366   \n",
       "worst perimeter                   0.529408         0.618344   \n",
       "worst area                        0.438296         0.543331   \n",
       "worst smoothness                  0.568187         0.518523   \n",
       "worst compactness                 1.000000         0.892261   \n",
       "worst concavity                   0.892261         1.000000   \n",
       "worst concave points              0.801080         0.855434   \n",
       "worst symmetry                    0.614441         0.532520   \n",
       "worst fractal dimension           0.810455         0.686511   \n",
       "target                           -0.590998        -0.659610   \n",
       "\n",
       "                         worst concave points  worst symmetry  \\\n",
       "mean radius                          0.744214        0.163953   \n",
       "mean texture                         0.295316        0.105008   \n",
       "mean perimeter                       0.771241        0.189115   \n",
       "mean area                            0.722017        0.143570   \n",
       "mean smoothness                      0.503053        0.394309   \n",
       "mean compactness                     0.815573        0.510223   \n",
       "mean concavity                       0.861323        0.409464   \n",
       "mean concave points                  0.910155        0.375744   \n",
       "mean symmetry                        0.430297        0.699826   \n",
       "mean fractal dimension               0.175325        0.334019   \n",
       "radius error                         0.531062        0.094543   \n",
       "texture error                       -0.119638       -0.128215   \n",
       "perimeter error                      0.554897        0.109930   \n",
       "area error                           0.538166        0.074126   \n",
       "smoothness error                    -0.102007       -0.107342   \n",
       "compactness error                    0.483208        0.277878   \n",
       "concavity error                      0.440472        0.197788   \n",
       "concave points error                 0.602450        0.143116   \n",
       "symmetry error                      -0.030413        0.389402   \n",
       "fractal dimension error              0.215204        0.111094   \n",
       "worst radius                         0.787424        0.243529   \n",
       "worst texture                        0.359755        0.233027   \n",
       "worst perimeter                      0.816322        0.269493   \n",
       "worst area                           0.747419        0.209146   \n",
       "worst smoothness                     0.547691        0.493838   \n",
       "worst compactness                    0.801080        0.614441   \n",
       "worst concavity                      0.855434        0.532520   \n",
       "worst concave points                 1.000000        0.502528   \n",
       "worst symmetry                       0.502528        1.000000   \n",
       "worst fractal dimension              0.511114        0.537848   \n",
       "target                              -0.793566       -0.416294   \n",
       "\n",
       "                         worst fractal dimension    target  \n",
       "mean radius                             0.007066 -0.730029  \n",
       "mean texture                            0.119205 -0.415185  \n",
       "mean perimeter                          0.051019 -0.742636  \n",
       "mean area                               0.003738 -0.708984  \n",
       "mean smoothness                         0.499316 -0.358560  \n",
       "mean compactness                        0.687382 -0.596534  \n",
       "mean concavity                          0.514930 -0.696360  \n",
       "mean concave points                     0.368661 -0.776614  \n",
       "mean symmetry                           0.438413 -0.330499  \n",
       "mean fractal dimension                  0.767297  0.012838  \n",
       "radius error                            0.049559 -0.567134  \n",
       "texture error                          -0.045655  0.008303  \n",
       "perimeter error                         0.085433 -0.556141  \n",
       "area error                              0.017539 -0.548236  \n",
       "smoothness error                        0.101480  0.067016  \n",
       "compactness error                       0.590973 -0.292999  \n",
       "concavity error                         0.439329 -0.253730  \n",
       "concave points error                    0.310655 -0.408042  \n",
       "symmetry error                          0.078079  0.006522  \n",
       "fractal dimension error                 0.591328 -0.077972  \n",
       "worst radius                            0.093492 -0.776454  \n",
       "worst texture                           0.219122 -0.456903  \n",
       "worst perimeter                         0.138957 -0.782914  \n",
       "worst area                              0.079647 -0.733825  \n",
       "worst smoothness                        0.617624 -0.421465  \n",
       "worst compactness                       0.810455 -0.590998  \n",
       "worst concavity                         0.686511 -0.659610  \n",
       "worst concave points                    0.511114 -0.793566  \n",
       "worst symmetry                          0.537848 -0.416294  \n",
       "worst fractal dimension                 1.000000 -0.323872  \n",
       "target                                 -0.323872  1.000000  \n",
       "\n",
       "[31 rows x 31 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 독립변수들 간의 상관관계 \n",
    "data_df.corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76049e21",
   "metadata": {},
   "source": [
    "## 훈련데이터, 테스트 데이터로 분류하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "34ac5a6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 30)\n",
      "(426,)\n",
      "(143, 30)\n",
      "(143,)\n"
     ]
    }
   ],
   "source": [
    "train_input, test_input, train_target,test_target = train_test_split(data,target,random_state=42)\n",
    "print(train_input.shape)\n",
    "print(train_target.shape)\n",
    "print(test_input.shape)\n",
    "print(test_target.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc7c6dc8",
   "metadata": {},
   "source": [
    "## 데이터 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "266fd96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "ss=StandardScaler()\n",
    "ss.fit(train_input , train_target)\n",
    "train_scaled = ss.transform(train_input)\n",
    "test_scaled = ss.transform(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e5a1167",
   "metadata": {},
   "source": [
    "## KNN 모델 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4ea26dbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN 최근접 모델\n",
      "훈련 :  0.9788732394366197\n",
      "테스트 :  0.958041958041958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\neighbors\\_classification.py:237: FutureWarning: Unlike other reduction functions (e.g. `skew`, `kurtosis`), the default behavior of `mode` typically preserves the axis it acts along. In SciPy 1.11.0, this behavior will change: the default value of `keepdims` will become False, the `axis` over which the statistic is taken will be eliminated, and the value None will no longer be accepted. Set `keepdims` to True or False to avoid this warning.\n",
      "  mode, _ = stats.mode(_y[neigh_ind, k], axis=1)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "kn = KNeighborsClassifier()\n",
    "kn.fit(train_scaled, train_target)\n",
    "print('KNN 최근접 모델')\n",
    "print('훈련 : ' ,kn.score(train_scaled, train_target))\n",
    "print('테스트 : ',kn.score(test_scaled, test_target))\n",
    "\n",
    "# [해석]\n",
    "# 정규화된 데이터로 결정계수를 확인\n",
    "# 훈련 :  0.9788732394366197\n",
    "# 테스트 :  0.958041958041958\n",
    "# 0.02 차이"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c601a08",
   "metadata": {},
   "source": [
    "## 로지스틱 회귀분류"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "c7b6f2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "로지스틱 회귀분류\n",
      "훈련 :  0.9859154929577465\n",
      "테스트 :  0.9790209790209791\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "lr.fit(train_scaled,train_target)\n",
    "\n",
    "print('로지스틱 회귀분류')\n",
    "print('훈련 : ' ,lr.score(train_scaled, train_target))\n",
    "print('테스트 : ',lr.score(test_scaled, test_target))\n",
    "\n",
    "# [해석]\n",
    "# 정규화된 데이터로 결정계수를 확인\n",
    "# 훈련 :  0.9859154929577465\n",
    "# 테스트 :  0.9790209790209791\n",
    "# 대략 0.01 차이 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56657d74",
   "metadata": {},
   "source": [
    "## 결정트리모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed1b2b21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "결정트리모델\n",
      "훈련 :  1.0\n",
      "테스트 :  0.951048951048951\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# 클래스 생성하기\n",
    "# - 실제사용시에는 randon_state는 사용하지 않는것이 좋다\n",
    "# - random_state값이 변경되면 정확도의 점수도 변경된다\n",
    "# 훈련모델 생성 \n",
    "dtc = DecisionTreeClassifier(random_state=42)\n",
    "dtc.fit(train_scaled,train_target)\n",
    "\n",
    "# 훈련 및 테스트 데이터 정확도 확인하기\n",
    "print('결정트리모델')\n",
    "print('훈련 : ',dtc.score(train_scaled,train_target))\n",
    "print('테스트 : ' ,dtc.score(test_scaled,test_target))\n",
    "\n",
    "# [해석]\n",
    "# 훈련 :  1.0\n",
    "# 테스트 :  0.951048951048951\n",
    "# 0.5 차이로 약간의 과대적합을 확인할수있다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3604c720",
   "metadata": {},
   "source": [
    "## 교차검증"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9aa81667",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 30) (455,)\n",
      "(114, 30) (114,)\n"
     ]
    }
   ],
   "source": [
    "# 훈련데이터와 테스트데이터를 8:2로 쪼개기 \n",
    "from sklearn.model_selection import train_test_split\n",
    "train_input, test_input, train_target,test_target = train_test_split(data,target,test_size=0.2,random_state=42)\n",
    "print(train_input.shape,train_target.shape)\n",
    "print(test_input.shape,test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a1f67a75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(364, 30) (364,)\n",
      "(91, 30) (91,)\n"
     ]
    }
   ],
   "source": [
    "### 2. 훈련데이터를 이용해서 검증데이터 생성하기\n",
    "sub_input, val_input, sub_target,val_target = train_test_split(train_input,train_target,test_size=0.2,random_state=42)\n",
    "print(sub_input.shape,sub_target.shape)\n",
    "print(val_input.shape,val_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "425f2b36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.9010989010989011\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier(random_state=42) # 괄호안에 넣는 값 사람이 지정 : 하이퍼파라미터, 정확도 높이기 위해 찾아내야한다 \n",
    "dtc.fit(sub_input, sub_target)\n",
    "print(dtc.score(sub_input, sub_target))\n",
    "print(dtc.score(val_input,val_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cc38a25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 30) (455,)\n",
      "(114, 30) (114,)\n"
     ]
    }
   ],
   "source": [
    "### 사용하는 데이터 다시확인\n",
    "# test 데이터는 마지막에 한번 사용 \n",
    "print(train_input.shape, train_target.shape)\n",
    "print(test_input.shape, test_target.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "837244a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'fit_time': array([0.00598478, 0.00398898, 0.00498486, 0.00498581, 0.00598502]), 'score_time': array([0.00070715, 0.0009985 , 0.        , 0.        , 0.        ]), 'test_score': array([0.92307692, 0.89010989, 0.93406593, 0.9010989 , 0.93406593])}\n",
      "0.9164835164835164\n"
     ]
    }
   ],
   "source": [
    "### 교차검증 모듈 : cross_validate\n",
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "# dtc : 결정트리 훈련모델(다른모델을 사용한 경우 해당모델 적어주면된다)\n",
    "# 두번째값 : 훈련데이터(fold에서 훈련데이터 쪼갤때 사용)\n",
    "# 세번째값 : 검증데이터(fold에서 검증데이터 쪼갤때 사용)\n",
    "scores = cross_validate(dtc,train_input,train_target)\n",
    "print(scores)\n",
    "# 기본값은 5폴드\n",
    "# test_score : 각 겹의 평균값\n",
    "\n",
    "## 딕셔너리의 test_score의 평균값이 최종 훈련모델의 평가점수(정확도)\n",
    "\n",
    "# 최종 훈련모델의 성능\n",
    "scores['test_score'].mean()\n",
    "\n",
    "\n",
    "# 다른방법\n",
    "sum=0\n",
    "for i in range (0,5,1) :\n",
    "    sum = scores['test_score'][i]+sum\n",
    "    \n",
    "print(sum/5)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1cb81b99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 :  0.9164835164835164\n"
     ]
    }
   ],
   "source": [
    "### 훈련데이터를 섞거나, 폴드의 갯수를 지정할 수 있는 클래스\n",
    "# - StratifiedKFold\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# 교차검증 함수 그대로 사용\n",
    "# cv : 분할기 속성\n",
    "#    : 분할기로 StratifiedKFold 클래스 사용\n",
    "#    : 속성값이 없을 경우 기본 fold 는 5, 기본 섞지는 않음 \n",
    "scores = cross_validate(dtc,train_input, train_target, cv=StratifiedKFold())\n",
    "\n",
    "print('최종 : ',np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "201b0eab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "최종 :  0.9186758893280633\n"
     ]
    }
   ],
   "source": [
    "# n_splits : fold 갯수 shuffle : 섞기\n",
    "splitter = StratifiedKFold(n_splits=20, shuffle = True,random_state=42)\n",
    "scores = cross_validate(dtc,train_input, train_target, cv=splitter)\n",
    "\n",
    "print('최종 : ',np.mean(scores['test_score']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cac613",
   "metadata": {},
   "source": [
    "## 확률적경사하강법(SGD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2a075112",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input, test_input, train_target,test_target = train_test_split(data,target,random_state=42)\n",
    "ss=StandardScaler()\n",
    "ss.fit(train_input , train_target)\n",
    "train_scaled = ss.transform(train_input)\n",
    "test_scaled = ss.transform(test_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "aa22b623",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "확률적 경사하강법(SGD)\n",
      "훈련 :  0.9812206572769953\n",
      "테스트 :  0.958041958041958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:705: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### 사용클래스(모델) : SGDClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# 클래스(모델)생성\n",
    "sc = SGDClassifier(loss='log', max_iter=10, random_state=42)\n",
    "\n",
    "# 훈련시키기\n",
    "sc.fit(train_scaled,train_target)\n",
    "\n",
    "# \n",
    "print('확률적 경사하강법(SGD)')\n",
    "print('훈련 : ',sc.score(train_scaled, train_target))\n",
    "print('테스트 : ',sc.score(test_scaled,test_target))\n",
    "\n",
    "# [해석]\n",
    "# 훈련 :  0.9812206572769953\n",
    "# 테스트 :  0.958041958041958\n",
    "# 0.03 차이 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d1513e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n",
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "sc = SGDClassifier(loss='log',random_state =42)\n",
    "\n",
    "train_score = []\n",
    "test_score = []\n",
    "\n",
    "# 범주 고유값 \n",
    "classes = np.unique(train_target)\n",
    "# 반복을 300회 이상으로 테스트하여 정확도를 리스트에 저장 \n",
    "for _ in range(0,300):\n",
    "    sc.partial_fit(train_scaled, train_target,classes=classes)\n",
    "    \n",
    "    train_score.append(sc.score(train_scaled,train_target))\n",
    "    test_score.append(sc.score(test_scaled,test_target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6bd3707c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 최적의 에포크 위치 확인하기 : 시각화\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.plot(train_score, label = 'train_score')\n",
    "plt.plot(test_score, label = 'test_score')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "00c92c3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "훈련 :  0.9882629107981221\n",
      "테스트 :  0.972027972027972\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\82102\\ml\\lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:173: FutureWarning: The loss 'log' was deprecated in v1.1 and will be removed in version 1.3. Use `loss='log_loss'` which is equivalent.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "### 사용클래스(모델) : SGDClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# 클래스(모델)생성\n",
    "# tol=None 모델이 스스로 찾는 최적값을 무시하고  우리가 지정해준 max_iter 값까지 수행하도록 \n",
    "sc = SGDClassifier(loss='log', max_iter=150, tol=None, random_state=42)\n",
    "\n",
    "# 훈련시키기\n",
    "sc.fit(train_scaled,train_target)\n",
    "\n",
    "print\n",
    "print('훈련 : ',sc.score(train_scaled, train_target))\n",
    "print('테스트 : ',sc.score(test_scaled,test_target))\n",
    "\n",
    "# [해석]\n",
    "# 훈련 :  0.9882629107981221\n",
    "# 테스트 :  0.972027972027972\n",
    "# 0.01 차이 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b99e8c",
   "metadata": {},
   "source": [
    "## < 트리의 앙상블 모델 > \n",
    "### - 랜덤포레스트"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "56f946d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트리의 앙상블 모델 (랜덤포레스트)\n",
      "1.0\n",
      "0.9577291381668946\n"
     ]
    }
   ],
   "source": [
    "# 랜덤포레스트 객체생성 : 코어 모두 사용\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(n_jobs= -1 , random_state=42)\n",
    "\n",
    "# 교차검증 진행\n",
    "# - return_train_score : 검증결과 반환받기\n",
    "from sklearn.model_selection import cross_validate\n",
    "scores = cross_validate(rfc,train_input, train_target, return_train_score = True, n_jobs=-1)\n",
    "\n",
    "# 최종 훈련평가 결과 및 검증결과\n",
    "print('트리의 앙상블 모델 (랜덤포레스트)')\n",
    "print(np.mean(scores['train_score']))\n",
    "print(np.mean(scores['test_score']))\n",
    "\n",
    "# [해석]\n",
    "# 훈련 :1.0\n",
    "# 훈련 : 0.9577291381668946\n",
    "# 대략 0.5차이\n",
    "# 약간의 과대적합 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020b5b5e",
   "metadata": {},
   "source": [
    "## < 트리의 앙상블 모델 > \n",
    "### - 엑스트라트리(ExtraTree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b7c8f445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트리의 앙상블 모델 (엑스트라트리)\n",
      "훈련 :  1.0\n",
      "테스트 ;  0.9623803009575924\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "etc = ExtraTreesClassifier(n_jobs= -1 , random_state=42)\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "scores = cross_validate(etc,train_input, train_target,return_train_score=True,n_jobs=-1)\n",
    "\n",
    "# 최종 훈련평가 결과 및 검증 결과\n",
    "print('트리의 앙상블 모델 (엑스트라트리)')\n",
    "print('훈련 : ',np.mean(scores['train_score']))\n",
    "print('테스트 ; ',np.mean(scores['test_score']))\n",
    "\n",
    "# [해석]\n",
    "# 훈련 :  1.0\n",
    "# 테스트 ;  0.9623803009575924\n",
    "# 0.04 차이 약간의 과대적합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c58a4a9",
   "metadata": {},
   "source": [
    "## < 트리의 앙상블 모델 > \n",
    "### - 그래디언트부스팅(GradientBoosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e4979205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "트리의 앙상블 모델(그래디언트 부스팅)\n",
      "훈련 :  1.0\n",
      "테스트 :  0.9506703146374831\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "gdc = GradientBoostingClassifier(random_state=42)\n",
    "\n",
    "from sklearn.model_selection import cross_validate\n",
    "scores = cross_validate(gdc,train_input, train_target,return_train_score=True,n_jobs=-1)\n",
    "\n",
    "# 최종 훈련평가 결과 및 검증 결과\n",
    "print('트리의 앙상블 모델(그래디언트 부스팅)')\n",
    "print('훈련 : ' ,np.mean(scores['train_score']))\n",
    "print('테스트 : ',np.mean(scores['test_score']))\n",
    "\n",
    "# [해석]\n",
    "# 훈련 :  1.0\n",
    "# 테스트 :  0.9506703146374831\n",
    "# 0.05차이 약간의 과대적합"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9412b522",
   "metadata": {},
   "source": [
    "## < 트리의 앙상블 모델 > \n",
    "### - 히스토그램기반 그래디언트부스팅(Histogram-base Gradient Boosting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "66fbea56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "히스토그램기반 그래디언트부스팅(Histogram-base Gradient Boosting)\n",
      "훈련 :  1.0\n",
      "테스트 :  0.9600820793433653\n"
     ]
    }
   ],
   "source": [
    "## 사용하는 클래스(모델) : HistGradientBoostingClassifier\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "hgbc = HistGradientBoostingClassifier(random_state=42)\n",
    "scores= cross_validate(hgbc,train_input, train_target,return_train_score=True,n_jobs=-1)\n",
    "print('히스토그램기반 그래디언트부스팅(Histogram-base Gradient Boosting)')\n",
    "print('훈련 : ' ,np.mean(scores['train_score']))\n",
    "print('테스트 : ',np.mean(scores['test_score']))\n",
    "\n",
    "# [해석]\n",
    "# 훈련 :  1.0\n",
    "# 테스트 :  0.9600820793433653\n",
    "# 0.04차이 약간의 과대 적합 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecc2438",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련모델 결정계수가 1인 값은 사용하지 않는다 \n",
    "\n",
    "# 정규화된 데이터로 결정계수를 확인\n",
    "# 훈련 :  0.9859154929577465\n",
    "# 테스트 :  0.9790209790209791\n",
    "# 대략 0.01 차이 \n",
    "\n",
    "#확률적경사하강법(SGD) 모델의 에포크 값 150으로 주었을때\n",
    "# 훈련 :  0.9882629107981221\n",
    "# 테스트 :  0.972027972027972\n",
    "# 0.01 차이 \n",
    "\n",
    "# 두가지 모델을 사용하는게 적합 하다고 생각한다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_kernel",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
